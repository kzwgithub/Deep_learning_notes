<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-1.Initialization_kzw" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/05/1.Initialization_kzw/" class="article-date">
  <time datetime="2018-12-05T12:12:55.836Z" itemprop="datePublished">2018-12-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="改善deep-NN-week1-assignment1"><a href="#改善deep-NN-week1-assignment1" class="headerlink" title="改善deep NN : week1,assignment1"></a>改善deep NN : week1,assignment1</h1><h2 id="1-神经网络中需要初始化的两种参数类型为"><a href="#1-神经网络中需要初始化的两种参数类型为" class="headerlink" title="1.神经网络中需要初始化的两种参数类型为"></a>1.神经网络中需要初始化的两种参数类型为</h2><p><img src="https://i.imgur.com/CcdQPWx.jpg" alt=""></p>
<h2 id="2-三种初始化参数的方法"><a href="#2-三种初始化参数的方法" class="headerlink" title="2.三种初始化参数的方法"></a>2.三种初始化参数的方法</h2><h3 id="1-零初始化"><a href="#1-零初始化" class="headerlink" title="1) 零初始化"></a>1) 零初始化</h3><h3 id="2-ranndom初始化"><a href="#2-ranndom初始化" class="headerlink" title="2) ranndom初始化"></a>2) ranndom初始化</h3><h3 id="3-He初始化"><a href="#3-He初始化" class="headerlink" title="3) He初始化"></a>3) He初始化</h3><h2 id="3-零初始化"><a href="#3-零初始化" class="headerlink" title="3.零初始化"></a>3.零初始化</h2><h3 id="1-具体操作"><a href="#1-具体操作" class="headerlink" title="1) 具体操作"></a>1) 具体操作</h3><ul>
<li>W = np.zeros((layers_dims[l],layers_dims[l-1]),dtype=float)</li>
<li>b = np.zeros((layers_dims[l],1),dtype=float)</li>
</ul>
<h3 id="2-实验结果-结论"><a href="#2-实验结果-结论" class="headerlink" title="2) 实验结果,结论"></a>2) 实验结果,结论</h3><ul>
<li>结果很糟糕,accuracy=0.5,网络并没有学到东西</li>
<li>一般情况下，初始化所有权值为零会导致网络不能打破对称性。这意味着每一层的每个神经元都会学到同样的东西</li>
<li>** 权重W应该随机初始化来打破对称性</li>
<li>**偏置b可以初始化为零,只要W打破了对称性</li>
</ul>
<h2 id="4-ranndom初始化-值较大时"><a href="#4-ranndom初始化-值较大时" class="headerlink" title="4.ranndom初始化,值较大时"></a>4.ranndom初始化,值较大时</h2><h3 id="1-具体操作-1"><a href="#1-具体操作-1" class="headerlink" title="1) 具体操作"></a>1) 具体操作</h3><ul>
<li>W = np.random.randn(layers_dims[l],layers_dims[l-1])*10</li>
<li>b = np.zeros((layers_dims[l],1),dtype=float)</li>
</ul>
<h3 id="2-实验结果-结论-1"><a href="#2-实验结果-结论-1" class="headerlink" title="2) 实验结果,结论"></a>2) 实验结果,结论</h3><ul>
<li>结果较好,accuracy=0.86</li>
<li>当随机初始化的参数值较大时,网络表现不是很好,建议将10换成0.01</li>
</ul>
<h2 id="5-He初始化"><a href="#5-He初始化" class="headerlink" title="5.He初始化"></a>5.He初始化</h2><h3 id="1-条件-使用ReLU激活函数"><a href="#1-条件-使用ReLU激活函数" class="headerlink" title="1)条件 : 使用ReLU激活函数"></a>1)条件 : 使用ReLU激活函数</h3><h3 id="2-系数图解"><a href="#2-系数图解" class="headerlink" title="2)系数图解"></a>2)系数图解</h3><p><img src="https://i.imgur.com/OK5uvEO.jpg" alt=""></p>
<h3 id="3-具体操作"><a href="#3-具体操作" class="headerlink" title="3) 具体操作"></a>3) 具体操作</h3><ul>
<li>W = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])</li>
<li>b = np.zeros((layers_dims[l],1),dtype=float)</li>
</ul>
<h3 id="4-实验结果-结论"><a href="#4-实验结果-结论" class="headerlink" title="4) 实验结果,结论"></a>4) 实验结果,结论</h3><ul>
<li>结果很不错,accuracy=0.96</li>
</ul>
<h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h2><ul>
<li>不同的初始化方式会得到不同的结果</li>
<li>随机初始化用来打破对称性,确保每个神经元都能学到不同的信息</li>
<li>不要初始化太大的值</li>
<li>He_initialization在使用ReLU激活函数时表现的很好</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/05/1.Initialization_kzw/" data-id="cjpb4x45v0000esg6ssxvu9vo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-assignment3_kzw" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/assignment3_kzw/" class="article-date">
  <time datetime="2018-12-04T08:09:44.291Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="任务3-单隐藏层实现数据分类"><a href="#任务3-单隐藏层实现数据分类" class="headerlink" title="任务3 : 单隐藏层实现数据分类"></a>任务3 : 单隐藏层实现数据分类</h1><h2 id="1-简单逻辑回归的概念理解"><a href="#1-简单逻辑回归的概念理解" class="headerlink" title="1.简单逻辑回归的概念理解"></a>1.简单逻辑回归的概念理解</h2><h3 id="简单逻辑回归适合线性可分的数据分类-如-一条直线将数据分成了两份"><a href="#简单逻辑回归适合线性可分的数据分类-如-一条直线将数据分成了两份" class="headerlink" title="简单逻辑回归适合线性可分的数据分类,如:一条直线将数据分成了两份"></a>简单逻辑回归适合线性可分的数据分类,如:一条直线将数据分成了两份</h3><h3 id="对于本身不是线性可分的数据集会显得力不从心"><a href="#对于本身不是线性可分的数据集会显得力不从心" class="headerlink" title="对于本身不是线性可分的数据集会显得力不从心"></a>对于本身不是线性可分的数据集会显得力不从心</h3><h2 id="2-逻辑回归的损失函数-交叉熵"><a href="#2-逻辑回归的损失函数-交叉熵" class="headerlink" title="2.逻辑回归的损失函数 : 交叉熵"></a>2.逻辑回归的损失函数 : 交叉熵</h2><p><img src="https://i.imgur.com/MsYoeZG.jpg" alt=""></p>
<h2 id="3-初始化模型的参数的方法"><a href="#3-初始化模型的参数的方法" class="headerlink" title="3.初始化模型的参数的方法"></a>3.初始化模型的参数的方法</h2><p><img src="https://i.imgur.com/5byFlDH.jpg" alt=""></p>
<h2 id="4-注意本课程中W-b的shape特点"><a href="#4-注意本课程中W-b的shape特点" class="headerlink" title="4.注意本课程中W,b的shape特点"></a>4.注意本课程中W,b的shape特点</h2><ul>
<li>1.输入值X的shape : (size_of_single_sample,num_of_samples)</li>
<li>2.参数W,b的shape : </li>
<li><img src="https://i.imgur.com/y0OwMRV.jpg" alt=""></li>
</ul>
<h2 id="5-Knowledge-extension"><a href="#5-Knowledge-extension" class="headerlink" title="5.Knowledge extension"></a>5.Knowledge extension</h2><ol>
<li>tan激活函数的求导 :<br><img src="https://i.imgur.com/lzBPPhP.jpg" alt=""></li>
<li>Python列表解析配合if else<br><img src="https://i.imgur.com/mBh1Hib.jpg" alt=""></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/04/assignment3_kzw/" data-id="cjpb4x4650002esg6nplg9wv3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-assignment2_2_note" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/assignment2_2_note/" class="article-date">
  <time datetime="2018-12-04T02:30:48.700Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="任务2-1-逻辑回归-用神经网络思维"><a href="#任务2-1-逻辑回归-用神经网络思维" class="headerlink" title="任务2_1 : 逻辑回归(用神经网络思维)"></a>任务2_1 : 逻辑回归(用神经网络思维)</h1><h2 id="1-数据的预处理-归一化"><a href="#1-数据的预处理-归一化" class="headerlink" title="1.数据的预处理,归一化"></a>1.数据的预处理,归一化</h2><h3 id="数据预处理的一般步骤"><a href="#数据预处理的一般步骤" class="headerlink" title="数据预处理的一般步骤:"></a>数据预处理的一般步骤:</h3><ol>
<li>确定数据集的demensions和shape(m_train, m_test, w_px, h_px …)</li>
<li>reshape数据集成一个向量:shape=(num_train, w_px, h_px, 3) –&gt; shape=(num_train, w_px <em> h_px </em> 3)<br><img src="https://i.imgur.com/werK0ce.jpg" alt=""></li>
</ol>
<h3 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化:"></a>数据归一化:</h3><ol>
<li>常见的数据归一化方式: 减去数组平均值 –&gt; 除以数组标准差</li>
<li>图片数据归一化 : 每个样本除以255（像素通道的最大值）<h3 id="断言的使用-对定向确定bug很有效"><a href="#断言的使用-对定向确定bug很有效" class="headerlink" title="断言的使用: 对定向确定bug很有效"></a>断言的使用: 对定向确定bug很有效</h3> assert(w.shape == (dim, 1))<br> assert(isinstance(b, float) or isinstance(b, int))</li>
</ol>
<h2 id="2-学习算法的一般结构"><a href="#2-学习算法的一般结构" class="headerlink" title="2.学习算法的一般结构"></a>2.学习算法的一般结构</h2><h3 id="结构"><a href="#结构" class="headerlink" title="结构:"></a>结构:</h3><p><img src="https://i.imgur.com/8v7MUvc.jpg" alt=""></p>
<h3 id="学习算法的实现过程"><a href="#学习算法的实现过程" class="headerlink" title="学习算法的实现过程:"></a>学习算法的实现过程:</h3><ol>
<li>初始化模型参数</li>
<li>通过最小化cost来学习模型的参数(training set)</li>
<li>使用学习到的参数做预测(test set)</li>
<li>分析结果,得出结论<h3 id="前向传播-二分类逻辑回归"><a href="#前向传播-二分类逻辑回归" class="headerlink" title="前向传播(二分类逻辑回归):"></a>前向传播(二分类逻辑回归):</h3><img src="https://i.imgur.com/yv47AKa.jpg" alt=""><h3 id="后向传播-二分类逻辑回归"><a href="#后向传播-二分类逻辑回归" class="headerlink" title="后向传播(二分类逻辑回归):"></a>后向传播(二分类逻辑回归):</h3><img src="https://i.imgur.com/NVLNaiD.jpg" alt=""></li>
</ol>
<h2 id="3-建立神经网络的一般步骤"><a href="#3-建立神经网络的一般步骤" class="headerlink" title="3.建立神经网络的一般步骤"></a>3.建立神经网络的一般步骤</h2><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤:"></a>步骤:</h3><ol>
<li>定义模型结构</li>
<li>初始化模型参数</li>
<li>迭代学习模型参数: 前向传播 -&gt; 计算cost -&gt; 后向传播 -&gt; 计算grad -&gt; 更新参数(梯度下降)</li>
<li>在test set上做预测<h3 id="one-more-thing"><a href="#one-more-thing" class="headerlink" title="one more thing:"></a>one more thing:</h3></li>
</ol>
<ul>
<li>通常都会把step1-3整合到一个函数中(nn_model),然后就可以:建立model -&gt; 学习参数 -&gt; 在新数据上做预测</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/04/assignment2_2_note/" data-id="cjpb4x4670003esg6bmzv7rc6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-assignment2_1_note" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/assignment2_1_note/" class="article-date">
  <time datetime="2018-12-04T02:30:48.695Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="任务2-1-Python-numpy-基础练习"><a href="#任务2-1-Python-numpy-基础练习" class="headerlink" title="任务2_1 :Python numpy 基础练习"></a>任务2_1 :Python numpy 基础练习</h1><h2 id="1-normalization"><a href="#1-normalization" class="headerlink" title="1.normalization"></a>1.normalization</h2><p>###示例:按行归一化<br>    x = [[0,3,4],[2,6,4]]<br>    x_norm = np.linalg.norm(x,ord=2,axis=1,keepdims=True) #=[5,根号(56)]<br>    x_normalized = x/x_norm = [[0,3/5,4/5],[2/根号(56),6/根号(56),4/根号(56)]]</p>
<p>###注意 : </p>
<ul>
<li>axis=1 : 按行归一化</li>
<li>keepdims : 保持矩阵原有的形状 -&gt; 便于brodacast</li>
<li>ord : 范数,默认是求2范数</li>
<li><p>向量的范数 : </p>
<p><img src="https://i.imgur.com/LEd3HZB.png" alt=""></p>
</li>
</ul>
<h2 id="2-the-gradient-of-sigmoid"><a href="#2-the-gradient-of-sigmoid" class="headerlink" title="2.the gradient of sigmoid"></a>2.the gradient of sigmoid</h2><p>###示例:<br>    sigmoid_derivative(x) = σ′(x) = σ(x)(1−σ(x))</p>
<h2 id="3-L1损失和L2损失"><a href="#3-L1损失和L2损失" class="headerlink" title="3.L1损失和L2损失"></a>3.L1损失和L2损失</h2><ul>
<li>L1损失 : </li>
</ul>
<p><img src="https://i.imgur.com/VHJwx0v.jpg" alt=""></p>
<ul>
<li>L2损失 : </li>
</ul>
<p><img src="https://i.imgur.com/Dz1q37K.jpg" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/04/assignment2_1_note/" data-id="cjpb4x4630001esg6xfjt0nhp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/05/1.Initialization_kzw/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/12/04/assignment3_kzw/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/12/04/assignment2_2_note/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/12/04/assignment2_1_note/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>